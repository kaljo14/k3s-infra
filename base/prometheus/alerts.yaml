apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-alerts
data:
  alerts.yml: |
    groups:
    - name: ingress_monitoring
      interval: 30s
      rules:
      - alert: IngressDown
        expr: probe_success{job="blackbox"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Ingress endpoint is down"
          description: "Ingress endpoint {{ $labels.instance }} has been down for more than 2 minutes."

      - alert: IngressHighLatency
        expr: probe_http_duration_seconds{job="blackbox"} > 2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Ingress endpoint has high latency"
          description: "Ingress endpoint {{ $labels.instance }} has latency above 2s for more than 5 minutes."

      - alert: IngressHighErrorRate
        expr: probe_http_status_code{job="blackbox"} >= 500
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Ingress endpoint returning errors"
          description: "Ingress endpoint {{ $labels.instance }} is returning 5xx errors."

    - name: prometheus_monitoring
      interval: 30s
      rules:
      - alert: PrometheusDown
        expr: up{job="prometheus"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Prometheus is down"
          description: "Prometheus instance has been down for more than 1 minute."

      - alert: PrometheusTargetDown
        expr: up == 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Prometheus target is down"
          description: "Target {{ $labels.job }}/{{ $labels.instance }} has been down for more than 5 minutes."

    - name: traefik_monitoring
      interval: 30s
      rules:
      - alert: TraefikHighErrorRate
        expr: sum(rate(traefik_entrypoint_requests_total{code=~"5.."}[5m])) > 10
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Traefik has high 5xx error rate"
          description: "Traefik is experiencing high 5xx error rate ({{ $value }} req/s) for more than 5 minutes."

      - alert: TraefikHighRequestLatency
        expr: histogram_quantile(0.99, sum(rate(traefik_entrypoint_request_duration_seconds_bucket[5m])) by (le, entrypoint)) > 1
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Traefik has high request latency"
          description: "Traefik p99 latency is above 1s for more than 10 minutes."

    - name: kubernetes_monitoring
      interval: 30s
      rules:
      - alert: PodCrashLooping
        expr: rate(kube_pod_container_status_restarts_total[15m]) > 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Pod is crash looping"
          description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping."

      - alert: HighMemoryUsage
        expr: (container_memory_usage_bytes / container_spec_memory_limit_bytes) > 0.9
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Container memory usage is high"
          description: "Container {{ $labels.namespace }}/{{ $labels.pod }}/{{ $labels.container }} memory usage is above 90%."

      - alert: HighCPUUsage
        expr: rate(container_cpu_usage_seconds_total[5m]) > 0.9
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Container CPU usage is high"
          description: "Container {{ $labels.namespace }}/{{ $labels.pod }}/{{ $labels.container }} CPU usage is above 90%."

